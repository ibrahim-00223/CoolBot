from dotenv import load_dotenv
import os
from langchain_community.vectorstores import FAISS
from langchain_mistralai import MistralAIEmbeddings, ChatMistralAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate

load_dotenv()

print("Chargement de la base vectorielle FAISS...")

# STEP 1 : Charger la base vectorielle existante
Mistral_API_KEY = os.getenv("MISTRAL_API_KEY")
embeddings = MistralAIEmbeddings(
    model="mistral-embed",
    mistral_api_key=Mistral_API_KEY
)

vector_store = FAISS.load_local(
    "data_cleaning/faiss_index", 
    embeddings,
    allow_dangerous_deserialization=True  # N√©cessaire pour FAISS
)
print("Base vectorielle charg√©e avec succ√®s.")

# STEP 2 : Configuration du retriever
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # R√©cup√®re les 5 chunks les plus pertinents
)

# STEP 3 : Cr√©ation du LLM
llm = ChatMistralAI(
    model="mistral-large-latest",
    mistral_api_key=Mistral_API_KEY,
    temperature=0.2  # Faible temp√©rature pour des r√©ponses pr√©cises
)

# STEP 4 : Cr√©ation du prompt template
template = """Tu es un assistant technique stagiaire qui aide les techniciens sur le terrain.

Ta mission : aider le technicien √† r√©soudre rapidement ses probl√®mes en te basant sur la documentation technique disponible.

CONSIGNES :
- Sois direct et pratique, pas de blabla inutile
- Si tu trouves l'info dans la doc, donne la solution √©tape par √©tape
- Si plusieurs solutions existent, propose-les toutes
- Indique toujours la source (nom du document et page) pour que le technicien puisse v√©rifier
- Si tu ne trouves pas l'info, dis-le clairement et sugg√®re o√π chercher
- Utilise un ton professionnel mais accessible, comme un coll√®gue serviable

DOCUMENTATION DISPONIBLE :
{context}

QUESTION DU TECHNICIEN : {question}

TA R√âPONSE (avec sources) :"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# STEP 5 : Fonction pour formater les documents r√©cup√©r√©s
def format_docs(docs):
    formatted = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'Inconnu')
        page = doc.metadata.get('page', 'N/A')
        formatted.append(f"[Document {i} - Source: {source}, Page: {page}]\n{doc.page_content}")
    return "\n\n---\n\n".join(formatted)

# STEP 6 : Cr√©ation de la cha√Æne RAG (Retrieval-Augmented Generation)
rag_chain = (
    {
        "context": retriever | format_docs, 
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

# STEP 7 : Interface conversationnelle
print("\n" + "="*60)
print("ü§ñ ASSISTANT TECHNIQUE RAG - Pr√™t √† r√©pondre !")
print("="*60)
print("Tapez 'exit' ou 'quit' pour quitter.\n")

while True:
    user_question = input("\n‚ùì Votre question : ")
    
    if user_question.lower() in ['exit', 'quit', 'q']:
        print("\nüëã Au revoir !")
        break
    
    if not user_question.strip():
        print("‚ö†Ô∏è  Veuillez poser une question.")
        continue
    
    print("\nüîç Recherche dans la documentation...")
    
    try:
        # R√©cup√©rer les documents pertinents (optionnel, pour debug)
        relevant_docs = retriever.invoke(user_question)
        print(f"üìö {len(relevant_docs)} documents pertinents trouv√©s.\n")
        
        # G√©n√©rer la r√©ponse
        response = rag_chain.invoke(user_question)
        
        print("üí¨ R√©ponse :")
        print("-" * 60)
        print(response)
        print("-" * 60)
        
    except Exception as e:
        print(f"‚ùå Erreur : {e}")